https://visualgo.net/en/sorting

=======>
a1.
Bubble Sort
    Bubble Sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order.
    It keeps "bubbling" the largest value to the end in each pass ‚Äî like bubbles rising in water üíß.

    How ?
    * Compare two adjacent elements.
    * Swap if the left is greater than the right.
    * Repeat until the array is sorted.

Algorithm 
    1. Start from the beginning of the array.
    2. Compare the current element with the next element.
    3. If the current element is greater than the next one, swap them.
    4. Move to the next pair of elements and repeat Step 2‚Äì3.
    5. After one full pass, the largest element will be at the end of the array.
    6. Repeat the above steps for the remaining (unsorted) portion of the array.
    7. Stop when no swaps are needed in a full pass (i.e., array is sorted).

Time Complexity
    Case    | Time
    Best    | O(n) (Already sorted)
    Average | O(n¬≤)
    Worst   | O(n¬≤)
    Space   | O(1)
<=======

=======>
a2.
Insertion Sort
    Insertion Sort builds the final sorted array one item at a time. It‚Äôs similar to how you might sort playing cards in your hand.

    How ?
    * Start from the second element (index 1).
    * Compare it with the elements before.
    * Shift all greater elements one position to the right.
    * Insert the current element in the correct position.

Time Complexity
    Case    | Time
    Best    | O(n) (alreay sorted)
    Average | O(n¬≤)
    Worst   | O(n¬≤)
    Space   | O(1)
<=======

=======>
a3.
Selection Sort
    Selection Sort is a simple comparison-based sorting algorithm.
    It works by repeatedly finding the minimum element from the unsorted part and putting it at the beginning.

    Why ?
    Easy to understand and implement.
    Good for small datasets.
    Does not require extra memory (in-place sorting).
    Not the most efficient for large arrays

    Time Complexity -(n¬≤) - for best, average, and worst
    space complexity - O(1) - no extra space used
    When to Use -- 	For small data or learning purposes
<=======

=======>
a4.
Merge Sort
    Merge Sort is a divide and conquer algorithm that:
    1. Divides the array into halves,
    2. Sorts each half recursively,
    3. Merges the sorted halves back together.

    Why ?
    Very efficient for large datasets.
    Stable sort (maintains order of equal elements).
    Consistent O(n log n) time complexity.
<=======

=======>
a5.
Quick Sort
    Quick Sort is a Divide & Conquer algorithm.
    It selects a pivot, partitions the array around it, and recursively sorts the sub-arrays.

    Why ?
    Efficient for large datasets
    Average Time: O(n log n)
    Worst-case: O(n¬≤) (rare, can be avoided with random pivots)
    In-place and fast in practice
<=======

=======>
a6.
Linear Search
    Linear Search is the simplest search algorithm.
    It checks each element one by one until the desired value is found or the array ends.

    Why ?
    Best for small datasets
    No sorting required
    Works on unsorted arrays
    Very easy to implement

    Summary 
    Feature         | Detail
    Type            | Search Algorithm
    Time Complexity | O(n)
    Space           | O(1)
    When to use     | Small or unsorted arrays
    Stable          | ‚úÖ Yes
    Simple          | ‚úÖ Very Easy
<=======

=======>
a7.
Binary Search
    Binary Search is an efficient algorithm to find an element in a sorted 
    array by repeatedly dividing the search space in half.

    Why ?
    Much faster than Linear Search for large, sorted arrays
    Time Complexity: O(log n)
    Requires the array to be sorted

    Summary 
    Feature         | Detail
    Type            | Search Algorithm
    Time Complexity | O(log n)
    Space           | O(1)
    Sorted Required | ‚úÖ Yes
    Faster than     | Linear Search on large datasets
<=======

=======>
a8.
Recursion
    Recursion is a technique where a function calls itself to solve smaller instances of a problem.

    Why ?
    To break down complex problems into simpler ones
    Useful for problems like tree traversal, factorial, fibonacci, etc.
    Makes code cleaner and shorter in some cases

    How ?
    A recursive function has:
    Base Case ‚Äì when to stop recursion
    Recursive Case ‚Äì when the function calls itself

    Summary 
    Concept               | Description
    Function calls itself | ‚úÖ Yes
    Base case needed      | ‚úÖ To stop recursion
    Useful for            | Tree, graphs, math problems
    Risk                  | Stack overflow (if not handled well)
<=======

=======>
a9.
Two Pointer Technique
    The Two Pointer Technique is a method where you use two pointers (variables) to traverse 
    data (like arrays) from different ends or directions.

    Why ?
    Efficient for sorted arrays
    Reduces time complexity
    Commonly used in:
        * Finding pairs
        * Reversing arrays
        * Removing duplicates
        * Merging arrays

    How ?
    One pointer starts at the beginning
    The other starts at the end (or another place)
    Move them based on some logic or condition

    Summary 
    Concept       | Explanation
    What          | Using two pointers to traverse array
    Use Cases     | Search, reverse, sort, remove duplicates
    Works best on | Sorted arrays or lists
    Benefit       | Efficient & easy to implement
<=======

=======>
b1.
Sliding Window Technique
    The Sliding Window technique is used to reduce the time complexity when dealing with 
    contiguous blocks of data in arrays or strings.

    Why ?
    * Efficient for problems with subarrays, substrings, or fixed-length patterns
    * Reduces nested loops ‚Üí better performance (often from O(n¬≤) to O(n))

    How ?
    You maintain a window (subset of elements) that "slides" over the data:
    * Keep track of a start and end
    * Move one or both to grow/shrink the window
    * Update the result based on current window

    Summary 
    Concept      | Explanation
    What         | A technique using a "window" (subarray or string)
    Use Cases    | Subarrays, substrings, longest/shortest patterns
    Benefits     | Improves performance, replaces nested loops
    Common Tasks | Max/min sum, unique chars, pattern matching
<=======

=======>
b2.
Hashing
    Hashing is the process of converting data (like a string or number) into a fixed-size value 
    (called a hash) ‚Äî usually done using a hash function.
    Think of it as:
    üîë Input ‚Üí hashFunction() ‚Üí üîí Hashed Value

    Why ?
    üîç Fast data access (constant time O(1))
    üîê Secure storage (like passwords)
    üí° Used in data structures like Hash Tables (i.e., Map or Object in JS)

    How ?
    You maintain a window (subset of elements) that "slides" over the data:
    * Keep track of a start and end
    * Move one or both to grow/shrink the window
    * Update the result based on current window

    Use Cases 
    Hash Maps / Hash Tables
    Checking duplicates in arrays
    Password storage with hashing
    Data lookup and retrieval

    Summary 
    üî∏ Feature | üîç Description
    What       | Converts input into a fixed-size output
    Why        | Fast lookups, duplicate checks, secure data
    In JS      | Objects, Maps, crypto module
    Use Cases  | Passwords, sets, dictionaries, caching
<=======